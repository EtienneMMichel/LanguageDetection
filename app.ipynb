{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "from src.dataset import load, preprocess\n",
    "from src import models\n",
    "from src import utils\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PCA_REDUCTION = 100\n",
    "SELECTED_LANG = [24, 31, 41, 32, 12, 45]\n",
    "NEPOCH = 20\n",
    "P_VALIDATION=.7\n",
    "cfg = {\n",
    "    \"num_layers\":2,\n",
    "    \"weight\":128\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Detection\n",
    "\n",
    "Cette étude présente le dataset WiLI et tente de développer un modèle performant "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation du dataset\n",
    "\n",
    "Le dataset est un extrait de différents paragraphes tirés de Wikipédia contenant 235 languages comprenant chaqun 1000 paragraphes. Le but est de détecter le language le plus présent dans un paragraphe.\n",
    "\n",
    "Ce dataset est présenté dans \"The WiLI benchmark dataset for written language identification\" de Martin Thoma. Dans notre cas particulier, nous étudierons pas les langages artificiels, tel que le HTML ou XML, ni les langues mortes, tel que le Latin.\n",
    "\n",
    "Dans la première partie, nous étudierons la classification de différents langages éloignés puis ensuite entre des langages sémantiquement rapprochés tel que l'Arabe et l'Arabe egyptien."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Défis\n",
    "\n",
    "Dans certains paragraphes, des caractères d'autres langages peuvent être présent. En citant un auteur étranger ou encore une référence à un nom commum etranger. Nous pouvons alors extraire les caratères les plus présents en les discrimiant par leur occurence comme indiqué par M. Thoma."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wili_2018 (C:/Users/etien/.cache/huggingface/datasets/wili_2018/WiLI-2018 dataset/1.1.0/78d7fe4a9d0a01168e45657f302c776ee0afc0978d44e2c3759f4c4975b845f5)\n",
      "100%|██████████| 2/2 [00:00<00:00, 80.95it/s]\n",
      "Loading cached processed dataset at C:\\Users\\etien\\.cache\\huggingface\\datasets\\wili_2018\\WiLI-2018 dataset\\1.1.0\\78d7fe4a9d0a01168e45657f302c776ee0afc0978d44e2c3759f4c4975b845f5\\cache-c149b0ff9e020866.arrow\n",
      "Loading cached processed dataset at C:\\Users\\etien\\.cache\\huggingface\\datasets\\wili_2018\\WiLI-2018 dataset\\1.1.0\\78d7fe4a9d0a01168e45657f302c776ee0afc0978d44e2c3759f4c4975b845f5\\cache-99e8eb6a5f027ac5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds:  Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 3000\n",
      "})\n",
      "test_ds:  Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 3000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "train_ds, test_ds = load.execute(selected_lang=SELECTED_LANG)\n",
    "print(\"train_ds: \", train_ds)\n",
    "print(\"test_ds: \", test_ds)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing\n",
    "\n",
    "Tout d'abord, nous allons enlever les caractères spéciaux, transformer en caractère minuscule, réduire les set de caractères puis garder uniquement les mots avec les caractères les plus communs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\etien\\.cache\\huggingface\\datasets\\wili_2018\\WiLI-2018 dataset\\1.1.0\\78d7fe4a9d0a01168e45657f302c776ee0afc0978d44e2c3759f4c4975b845f5\\cache-23c54f311f7dbf27.arrow\n",
      "Loading cached processed dataset at C:\\Users\\etien\\.cache\\huggingface\\datasets\\wili_2018\\WiLI-2018 dataset\\1.1.0\\78d7fe4a9d0a01168e45657f302c776ee0afc0978d44e2c3759f4c4975b845f5\\cache-1b15aa0aab5b69ad.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing:  Say San Jose et kumalima ya klase baley ed luyag na Northern Samar, Pilipinas. Unong ed 2010 Census, say papulasyon to et 16,079 totoo. Walay kabaleg tan sukat to ya 29.85 sq. km. Sikato et walad unaan ya distrito. Say zip code to et 6402.\n",
      "after removing:  say san jose et kumalima ya klase baley ed luyag na northern samar pilipinas. unong ed  census say papulasyon to et  totoo. walay kabaleg tan sukat to ya . sq. km. sikato et walad unaan ya distrito. say zip code to et .\n"
     ]
    }
   ],
   "source": [
    "print(\"before removing: \", train_ds[0]['sentence'])\n",
    "\n",
    "train_ds = preprocess.remove_special_char(train_ds)\n",
    "train_ds = preprocess.lower_sentences(train_ds)\n",
    "\n",
    "print(\"after removing: \", train_ds[0]['sentence'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer ces transformations nous utilisons le traitement par batch du Dataset d'Huggingface."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Pour vectorizer notre jeu de données, M. Thoma propose la mise en place d'un BagOfWord (BoW) pour compter les occurences des mots.\n",
    "De plus, nous appliquons une analyse de composantes principales (PCA) pour réduire l'espace d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_ratio_:  0.5652721811929179\n"
     ]
    }
   ],
   "source": [
    "train_ds, input_shape = preprocess.vectorize(train_ds, pca_reduction=PCA_REDUCTION)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analyse en composante principale pour N=100 conserve 56,5% de la variance expliquée ce qui est suivant pour expliquer le jeu de données dans le cadre de la classification de langages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally set format to be compatible with pytorch and define validation loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.set_format(train_ds)\n",
    "length_train = int(P_VALIDATION*len(train_ds))\n",
    "length_valid = int((1-P_VALIDATION)*len(train_ds))\n",
    "while(length_train + length_valid) < len(train_ds):\n",
    "    length_train +=1\n",
    "\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "        train_ds, [length_train, length_valid], generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE)\n",
    "valid_loader = DataLoader(val_subset, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define model\n",
    "Dans notre cas, nous allons implémenter un modèle récurrent LSTM et un modèle FeedForward en temps que benchmarck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.FeedForwardModel(cfg=cfg, input_size=input_shape, num_classes=len(SELECTED_LANG))\n",
    "# model = models.LSTM(cfg=cfg, input_size=input_shape, num_classes=len(SELECTED_LANG))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "Etant donnés qu'il s'agit d'une classification multiclass, nous choisissons la CrossEntropyLoss en adaptant l'encodage des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Pour simplifier l'étude, nous fixons comme optimizer un optimizer Adam avec un learning rate de 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.76: : 17it [00:00, 26.45it/s]\n",
      "valid_loss : 1.68, accuracy: 0.29:   5%|▌         | 1/20 [00:00<00:17,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.57: : 17it [00:00, 30.73it/s]\n",
      "valid_loss : 1.38, accuracy: 0.64:  10%|█         | 2/20 [00:01<00:14,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.30: : 17it [00:00, 26.61it/s]\n",
      "valid_loss : 1.16, accuracy: 0.90:  15%|█▌        | 3/20 [00:02<00:14,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.13: : 17it [00:00, 27.46it/s]\n",
      "valid_loss : 1.09, accuracy: 0.98:  20%|██        | 4/20 [00:03<00:13,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.09: : 17it [00:00, 28.43it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  25%|██▌       | 5/20 [00:04<00:13,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.08: : 17it [00:00, 29.14it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  30%|███       | 6/20 [00:05<00:11,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.08: : 17it [00:00, 27.24it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  35%|███▌      | 7/20 [00:05<00:11,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 29.42it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  40%|████      | 8/20 [00:06<00:10,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 28.08it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  45%|████▌     | 9/20 [00:07<00:09,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 26.47it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  50%|█████     | 10/20 [00:08<00:08,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 27.57it/s]\n",
      "valid_loss : 1.07, accuracy: 0.99:  55%|█████▌    | 11/20 [00:09<00:07,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 28.28it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  60%|██████    | 12/20 [00:10<00:06,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 26.49it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  65%|██████▌   | 13/20 [00:11<00:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 29.03it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  70%|███████   | 14/20 [00:11<00:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 26.17it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  75%|███████▌  | 15/20 [00:12<00:04,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 27.20it/s]\n",
      "valid_loss : 1.08, accuracy: 0.99:  80%|████████  | 16/20 [00:13<00:03,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 28.12it/s]\n",
      "valid_loss : 1.07, accuracy: 0.99:  85%|████████▌ | 17/20 [00:14<00:02,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 27.17it/s]\n",
      "valid_loss : 1.07, accuracy: 0.99:  90%|█████████ | 18/20 [00:15<00:01,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 26.61it/s]\n",
      "valid_loss : 1.07, accuracy: 0.99:  95%|█████████▌| 19/20 [00:16<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss : 1.07: : 17it [00:00, 28.93it/s]\n",
      "valid_loss : 1.07, accuracy: 0.99: 100%|██████████| 20/20 [00:16<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in (pbar := tqdm.tqdm(range(NEPOCH))):\n",
    "        # Train 1 epoch\n",
    "        train_loss = utils.train(model, train_loader, f_loss, optimizer, device)\n",
    "\n",
    "        # Test\n",
    "        valid_loss,accuracy = utils.test(model, valid_loader, f_loss, device)\n",
    "        print(\"-----------------\")\n",
    "        pbar.set_description(f\"valid_loss : {valid_loss:.2f}, accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\etien\\.cache\\huggingface\\datasets\\wili_2018\\WiLI-2018 dataset\\1.1.0\\78d7fe4a9d0a01168e45657f302c776ee0afc0978d44e2c3759f4c4975b845f5\\cache-a363a2486176307d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\etien\\.cache\\huggingface\\datasets\\wili_2018\\WiLI-2018 dataset\\1.1.0\\78d7fe4a9d0a01168e45657f302c776ee0afc0978d44e2c3759f4c4975b845f5\\cache-bdea94aca1829d8e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 1.08, accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "test_ds = preprocess.remove_special_char(test_ds)\n",
    "test_ds = preprocess.lower_sentences(test_ds)\n",
    "\n",
    "test_ds, input_shape = preprocess.vectorize(test_ds, pca_reduction=PCA_REDUCTION, from_save=True)\n",
    "preprocess.set_format(test_ds)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "test_loss, accuracy = utils.test(model, test_loader, f_loss, device)\n",
    "print(f\"test_loss: {test_loss:.2f}, accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Les deux modèles étudiés performent de manière similaire sur un ensemble de langues de taille 6. Pour la suite, il serait pertinent de les comparer sur des langages très rapprochées et sur un ensemble plus élargie.\n",
    "\n",
    "De plus pour comparer correctement les modèles, il aurait fallut utiliser WandB pour mieux visualiser les performances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
